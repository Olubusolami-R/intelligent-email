{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip3 install -U git+https://github.com/huggingface/accelerate.git","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, AutoConfig\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/enron/enron7.csv')\ndf.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_df=df.sample(n = 200, random_state=42)\ndf = df.drop(sampled_df.index)\nsampled_df = sampled_df.reset_index(drop=True)\ndf = df.reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# special tokens are defined\nbos = '<|endoftext|>'\neos = '<|EOS|>'\nbody = '<|body|>'\nadditional_special_tokens = [body]\n\nspecial_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': '<pad>',\n                       'sep_token': body} \n                      #  'additional_special_tokens':additional_special_tokens}\n\n# the new token is added to the tokenizer\nnum_added_toks = base_tokenizer.add_special_tokens(special_tokens_dict)\n\n# model configuration to which we add the special tokens\nconfig = AutoConfig.from_pretrained('gpt2', \n                                    bos_token_id=base_tokenizer.bos_token_id,\n                                    eos_token_id=base_tokenizer.eos_token_id,\n                                    pad_token_id=base_tokenizer.pad_token_id,\n                                    sep_token_id=base_tokenizer.sep_token_id,\n                                    output_hidden_states=False)\n\n# we load the pre-trained model with custom settings\nbase_model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n\n# model embeding resizing\nbase_model.resize_token_embeddings(len(base_tokenizer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Received'] = df['Received'].astype(str)\ndf['Response'] = df['Response'].astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_sequence_length = 512  # Maximum sequence length\n\nprepare_text = lambda x: ' '.join([bos, x['Received'], body, x['Response'], eos])[:max_sequence_length]\ndf['text'] = df.apply(prepare_text, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_news, df_val_news = train_test_split(df, train_size = 0.9, random_state = 77)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(df_train_news[['text']])\nval_dataset = Dataset.from_pandas(df_val_news[['text']])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n        return base_tokenizer(examples['text'], padding=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_dataset = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    num_proc=1\n)\n\ntokenized_val_dataset = val_dataset.map(\n    tokenize_function,\n    batched=True,\n    num_proc=1\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = './email_v2'\n\ntraining_args = TrainingArguments(\n    output_dir=model_path,          # output directory\n    num_train_epochs=20,              # total # of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=32,   # batch size for evaluation\n    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir=model_path,            # directory for storing logs\n    prediction_loss_only=True,\n    save_steps=10000\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n        tokenizer=base_tokenizer,\n        mlm=False\n    )\n\ntrainer = Trainer(\n    model=base_model,                         # the instantiated  Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    data_collator=data_collator,\n    train_dataset=tokenized_train_dataset,         # training dataset\n    eval_dataset=tokenized_val_dataset,            # evaluation dataset\n    \n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()\nbase_tokenizer.save_pretrained(model_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pretty_print(text, max_len_line=100):\n    words = text.split(' ')\n    len_line = 0\n    line = ''\n    for w in words:\n        if w == '\\n':\n            print(line)\n            line = ''\n            continue\n        if (len(line) + len(w)) > max_len_line:\n            print(line)\n            line = ''\n        line += ' ' + w\n    print(line)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_n_text_samples(model, tokenizer, input_text, device, n_samples = 5):\n    text_ids = tokenizer.encode(input_text, return_tensors = 'pt')\n    text_ids = text_ids.to(device)\n    model = model.to(device)\n\n    generated_text_samples = model.generate(\n        text_ids, \n        max_length= 512,  \n        num_return_sequences= n_samples,\n        no_repeat_ngram_size= 2,\n        repetition_penalty= 1.5,\n        top_p= 0.92,\n        temperature= .85,\n        do_sample= True,\n        top_k= 125,\n        early_stopping= True\n    )\n    gen_text = []\n    for t in generated_text_samples:\n        text = tokenizer.decode(t, skip_special_tokens=True)\n        gen_text.append(text)\n\n        return gen_text\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():  \n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\"  \ndevice = torch.device(dev)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trained model loading\nmodel_path = './email_v1'\n\nmy_model = GPT2LMHeadModel.from_pretrained(model_path)\nmy_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n\nbos = my_tokenizer.bos_token\neos = my_tokenizer.eos_token\nsep = my_tokenizer.sep_token\n\nemails = {}\n# prompts=[\n#     \"Hello! I hope this email finds you well. I am reaching out to seek permission for a university-sponsored travel opportunity. The trip is related to [purpose] and I believe it would greatly contribute to my academic and personal growth. I kindly request your approval for this travel. Best regards.\",\n#     \"Good morning, sir/ma'am! I am writing to inform you about a situation where certain resources in the library have been found spoilt or damaged. I kindly request your intervention in addressing this issue and ensuring that the necessary repairs or replacements are made. Thank you for your attention to this matter. Regards.\",\n#     \"Hello! I hope you're having a productive day. I am reaching out to highlight a lack of resources in [specific area/department]. This shortage is hindering our ability to effectively carry out academic activities. I kindly request your assistance in addressing this issue and providing the necessary resources. Best regards.\",\n#     \"Good morning, sir/ma'am! I am writing to seek permission for [specific activity/event] that is essential for my academic progress. I kindly request your approval for this activity as it would greatly contribute to my learning experience. Thank you for your understanding and support. Regards.\",\n#     \"Hello! I hope this email finds you well. I am reaching out to bring to your attention the need for additional resources in the [specific area/department]. The current shortage is adversely affecting our ability to perform our duties. I kindly request your prompt action in resolving this matter. Best regards.\",\n#     \"Good morning, sir/ma'am! I am writing to request permission to use a university facility for [specific purpose/activity]. I believe this activity is important for the student body and would greatly benefit the community. I kindly request your approval for this request. Thank you. Regards.\",\n#     \"Hello! I hope you're having a wonderful day. I am reaching out to report the lack of proper equipment in [specific area/department]. This shortage is impeding our ability to conduct practical sessions effectively. I kindly request your attention to this matter and prompt action in providing the necessary equipment. Best regards.\",\n#     \"Hello! Can you guide me on the textbooks I should obtain for the course 'Artificial Intelligence'? Best regards.\",\n#     \"Good morning, sir/ma'am! I'm curious to know about the textbooks required for the course 'Introduction to Sociology.' Can you assist me? Cheers!\",\n#     \"Dear Sir/Ma'am, could you please provide me with the information on the textbooks required for the course 'Introduction to Sociology'? Kind regards.\",\n#     \"Hello there! Can you inform me about the mandatory textbooks for the course 'Introduction to Sociology'? Best wishes.\",\n#     \"Good morning, sir/ma'am! I need information on the textbooks I must have for the course 'Introduction to Sociology.' Can you help me with that? Regards.\",\n#     \"My dear, how far everything? Don't play too much.\",\n#     \"Hello, I am a freelance writer interested in contributing to your magazine. Can you please provide me with the submission guidelines and any specific topics of interest? Thank you. Best regards.\",\n# ]\nprompts=[\n    \"Could you please do me a favor ? I would like to read your current title policy to see what it says about easements . You should have received a copy during your closing . \",\n    \"How are you doing? I have been trying to reach you. What's up?\",\n]\n\nfor p in prompts:\n\n    prompt = ' '.join([bos, p,  sep])\n    content = generate_n_text_samples(my_model, my_tokenizer, prompt, \n                                      device, n_samples = 1)[0]\n    emails[p] = content.replace(p, '')\n\nfor prompt, response in emails.items():\n    print('\\033[1m' + prompt + '\\033[0m')\n    pretty_print(response)\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nemails = {}\nprompts = sampled_df['Received']\n\nfor p in prompts:\n    prompt = ' '.join([bos, p, sep])\n    content = generate_n_text_samples(my_model, my_tokenizer, prompt, device, n_samples=1)[0]\n    emails[p] = content.replace(p, '')\n\n# Create a new column 'ai_response' in 'sampled_df' with the generated responses\nsampled_df['ai_response'] = sampled_df['Received'].map(emails)\n\n# Calculate BLEU score or perform further evaluation using 'sampled_df['ai_response']\n\n# Print the prompts and generated responses\nfor prompt, response in emails.items():\n    print('\\033[1m' + prompt + '\\033[0m')\n    pretty_print(response)\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statistics\nfrom nltk.translate.bleu_score import sentence_bleu\n\nscores=[]\n\nfor i in range(len(sampled_df)):\n  reference = sampled_df['Response'][i]\n  candidate = sampled_df['ai_response'][i]\n  scores.append(sentence_bleu(reference, candidate))\n\nprint(statistics.mean(scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ndef zip_dir(directory = os.curdir, file_name = 'file.zip'):\n    \"\"\"\n    zip all the files in a directory\n    \n    Parameters\n    _____\n    directory: str\n        directory needs to be zipped, defualt is current working directory\n        \n    file_name: str\n        the name of the zipped file (including .zip), default is 'directory.zip'\n        \n    Returns\n    _____\n    Creates a hyperlink, which can be used to download the zip file)\n    \"\"\"\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_dir()","metadata":{},"execution_count":null,"outputs":[]}]}